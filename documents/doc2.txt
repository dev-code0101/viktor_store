Summary:
# Title
CPPO: Accelerating the Training of Group Relative Policy
  Optimization-Based Reasoning Models

# Authors
Zhihang Lin, Mingbao Lin, Yuan Xie, Rongrong Ji

# Abstract
This paper introduces Completion Pruning Policy Optimization (CPPO) to
accelerate the training of reasoning models based on Group Relative Policy
Optimization (GRPO). GRPO, while effective, incurs high training costs due to
the need for sampling multiple completions for each question. Our experiment
and theoretical analysis reveals that the number of completions impacts model
accuracy yet increases training time multiplicatively, and not all completions
contribute equally to policy training -- their contribution depends on their
relative advantage. To address these issues, we propose CPPO, which prunes
completions with low absolute advantages, significantly reducing the number
needed for gradient calculation and updates. Additionally, we introduce a
dynamic completion allocation strategy to maximize GPU utilization by
incorporating additional questions, further enhancing training efficiency.
Experimental results demonstrate that CPPO achieves up to $8.32\times$ speedup
on GSM8K and $3.51\times$ on Math while preserving or even enhancing the
accuracy compared to the original GRPO. We release our code at
https://github.com/lzhxmu/CPPO.

# Categories
cs.AI

# Publication Details
- Published: March 28, 2025
- arXiv ID: 2503.22342v1



# BibTeX
@misc{lin2025cppoacceleratingtraininggroup,
      title={CPPO: Accelerating the Training of Group Relative Policy Optimization-Based Reasoning Models}, 
      author={Zhihang Lin and Mingbao Lin and Yuan Xie and Rongrong Ji},
      year={2025},
      eprint={2503.22342},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2503.22342}, 
}



Paper Content:
1.  1 Introduction [https://arxiv.org/html/2503.22342v1#S1]
 2.  2 Related Work [https://arxiv.org/html/2503.22342v1#S2]
 3.
     3 Method [https://arxiv.org/html/2503.22342v1#S3]
     1. 3.1 Preliminary [https://arxiv.org/html/2503.22342v1#S3.SS1]
     2. 3.2 Completion Contribution Analysis [https://arxiv.org/html/2503.22342v1#S3.SS2]
     3. 3.3 Completions Pruning Policy Optimization [https://arxiv.org/html/2503.22342v1#S3.SS3]
     4. 3.4 Parallel Processing through Dynamic Completion Allocation [https://arxiv.org/html/2503.22342v1#S3.SS4]
 4.
     4 Experiments [https://arxiv.org/html/2503.22342v1#S4]
     1. 4.1 Experimental Settings [https://arxiv.org/html/2503.22342v1#S4.SS1]
     2.
        4.2 Main Results [https://arxiv.org/html/2503.22342v1#S4.SS2]
        1. 4.2.1 Performance Comparison [https://arxiv.org/html/2503.22342v1#S4.SS2.SSS1]
        2. 4.2.2 An In-depth Analysis of CPPO’s Higher Accuracy [https://arxiv.org/html/2503.22342v1#S4.SS2.SSS2]
     3. 4.3 Ablation Study [https://arxiv.org/html/2503.22342v1#S4.SS3]
     4. 4.4 Case Study [https://arxiv.org/html/2503.22342v1#S4.SS4]
 5.  5 Limitations and Future Work [https://arxiv.org/html/2503.22342v1#S5]
 6.  6 Conclusion [https://arxiv.org/html/2503.22342v1#S6]
 7.  7 Acknowledgments [https://arxiv.org/html/2503.22342v1#S7]
 8.  A Specific Design of the Reward Function [https://arxiv.org/html/2503.22342v1#A1]
 9.  B Templates of Completion Pruning Policy Optimization [https://arxiv.org/html/2503.22342v1#A2]
 10. C More Case Studies [https://arxiv.org/html/2503.22342v1#A3]
CPPO: ACCELERATING THE TRAINING OF GROUP RELATIVE POLICY OPTIMIZATION-BASED REASONING MODELS
Zhihang Lin1,2,
Mingbao Lin3,
Yuan Xie2,4,
Rongrong Ji1
1Key Laboratory of Multimedia Trusted Perception and Efficient Computing,
Ministry of Education of China, Xiamen University, 361005, P.R. China
2Shanghai Innovation Institute, China
3Skywork AI, Singapore
4East China Normal University, Shanghai, China
lzhedu@foxmail.com, linmb001@outlook.com, yxie@cs.ecnu.edu.cn, rrji@xmu.edu.cn
Project LeaderCorresponding Author
ABSTRACT
This paper introduces Completion Pruning Policy Optimization (CPPO) to accelerate the training of reasoning models based on Group Relative Policy Optimization (GRPO).
GRPO, while effective, incurs high training costs due to the need for sampling multiple completions for each question.
Our experiment and theoretical analysis reveals that the number of completions impacts model accuracy yet increases training time multiplicatively, and not all completions contribute equally to policy training—their contribution depends on their relative advantage.
To address these issues, we propose CPPO, which prunes completions with low absolute advantages, significantly reducing the number needed for gradient calculation and updates.
Additionally, we introduce a dynamic completion allocation strategy to maximize GPU utilization by incorporating additional questions, further enhancing training efficiency.
Experimental results demonstrate that CPPO achieves up to $8.32\times$ speedup on GSM8K and $3.51\times$ on Math while preserving or even enhancing the accuracy compared to the original GRPO.
We release our code at https://github.com/lzhxmu/CPPO [https://github.com/lzhxmu/CPPO].
1 INTRODUCTION
Recently, there has been a surge in the development and application of advanced reasoning models, with models such as OpenAI-o1 [9 [https://arxiv.org/html/2503.22342v1#bib.bib9]], Deepseek-R1 [6 [https://arxiv.org/html/2503.22342v1#bib.bib6]], and Kimi-k1.5 [20 [https://arxiv.org/html/2503.22342v1#bib.bib20]] being prime examples.
These models exhibit remarkable capability in complex reasoning tasks, such as mathematics, coding, and scientific reasoning through step-by-step inference and reflection.
Reinforcement learning has been proven to be an effective method for training reasoning models.
Deepseek-R1 [6 [https://arxiv.org/html/2503.22342v1#bib.bib6]] demonstrates that reasoning patterns can be effectively elicited through rule-based reinforcement learning. It employs Group Relative Policy Optimization (GRPO) [17 [https://arxiv.org/html/2503.22342v1#bib.bib17]], which differs from Proximal Policy Optimization (PPO) [16 [https://arxiv.org/html/2503.22342v1#bib.bib16]] by estimating the baseline directly from group scores, eliminating the need for a critic model.
However, this necessitates sampling a group of completions for each question, rendering the training process computationally expensive.
Subsequently, GRPO computes the reward for each completion using a rule-based reward function and calculates the relative advantage of each completion.
To ensure training stability, GRPO also calculates the ratio of the predicted probabilities of the policy model, reference model, and old policy model for a group of completions as part of the policy objective function, further increasing the training overhead of reinforcement learning.
The substantial training overhead of GRPO limits its training efficiency and scalability.
Improving the training efficiency is an important and practical problem.
The computational expense of GRPO training primarily stems from its core design: generating a large group of completions per prompt for intra-group comparison, which makes the training process computationally expensive.
Moreover, the forward computation of GRPO scales by a factor of (3$\times$) completion number.
It is natural to question whether the contribution of each completion to the reinforcement learning process is equal. In Sec. 3.2 [https://arxiv.org/html/2503.22342v1#S3.SS2], we find that the contribution of each completion is related to its relative advantage.
In other words, the contribution of each completion to the policy model training is not equal.
This insight inspires us to accelerate GRPO by pruning completions.
In this paper, we propose Completion Pruning Policy Optimization (CPPO) to accelerate Group Relative Policy Optimization (GRPO).
Given that each completion’s contribution to the reinforcement learning process varies significantly and is closely related to its relative advantage, our CPPO prunes completions based on advantage, thereby accelerating the reinforcement learning process.
Specifically, the policy model initially samples a group of completions for each question.
Subsequently, the relative advantage of each completion is computed via the reward function.
CPPO then prunes completions with low absolute advantage values, retaining only those with high absolute advantage for loss computation.
This process considerably reduces the number of completions needed for training, thus speeding up the training process.
Moreover, we observe underutilized GPU resources due to completion pruning, leading to resource waste.
To tackle this, we introduce a dynamic completion allocation strategy that fills each device with completions from new questions, fully utilizing GPU resources and further enhancing training efficiency.
We have conducted experiments on multiple challenging benchmarks and models of different scales to evaluate CPPO’s effectiveness.
Specifically, we train the Qwen-2.5 series models [24 [https://arxiv.org/html/2503.22342v1#bib.bib24]], such as Qwen-2.5-1.5B-Instruct and Qwen-2.5-7B-Instruct, on math datasets including Math [7 [https://arxiv.org/html/2503.22342v1#bib.bib7]] and GSM8K [3 [https://arxiv.org/html/2503.22342v1#bib.bib3]].
The results demonstrate that CPPO achieves up to $8.32\times$ speedup on GSM8K and $3.51\times$ on Math while preserving or even enhancing the accuracy compared to the original GRPO.
2 RELATED WORK
Large Scale Reasoning Models.
Large Language Models (LLM) [1 [https://arxiv.org/html/2503.22342v1#bib.bib1]; 21 [https://arxiv.org/html/2503.22342v1#bib.bib21]; 19 [https://arxiv.org/html/2503.22342v1#bib.bib19]; 2 [https://arxiv.org/html/2503.22342v1#bib.bib2]] have made impressive progress in various natural language processing tasks.
Recently, researchers continue to boost the performance of large language models in reasoning tasks, such as mathematics [3 [https://arxiv.org/html/2503.22342v1#bib.bib3]; 7 [https://arxiv.org/html/2503.22342v1#bib.bib7]], coding [10 [https://arxiv.org/html/2503.22342v1#bib.bib10]], and scientific reasoning [15 [https://arxiv.org/html/2503.22342v1#bib.bib15]].
Snell et al. [18 [https://arxiv.org/html/2503.22342v1#bib.bib18]] use dense, process-based verifier reward models and adaptively update the model’s response distribution based on the test-time prompt to enhance reasoning ability.
rStar-Math [5 [https://arxiv.org/html/2503.22342v1#bib.bib5]] proposes a self-evolved deep thinking approach that significantly boosts the math reasoning capabilities of small LLMs.
OpenAI-o1 [9 [https://arxiv.org/html/2503.22342v1#bib.bib9]] uses large scale reinforcement learning to train a reasoning model that can solve complex reasoning tasks, achieving state-of-the-art performance on multiple benchmarks.
However, the training details of OpenAI-o1 have not been released, making it difficult to replicate and expand the reasoning model.
Reinforcement Learning.
Recently, DeepSeek-R1 [6 [https://arxiv.org/html/2503.22342v1#bib.bib6]] has incentivized the reasoning capability of large language models through Group Relative Policy Optimization. Inspired by DeepSeek-R1’s success, Logic-RL [23 [https://arxiv.org/html/2503.22342v1#bib.bib23]] adopts the REIFORCE++ algorithm to enhance the training efficiency and stability of rule-based reinforcement learning. Hu et al. [8 [https://arxiv.org/html/2503.22342v1#bib.bib8]] demonstrate that the vanilla PPO algorithm, without KL divergence constraint, is sufficient to scale up both response length and benchmark performance on reasoning tasks. Nevertheless, these reinforcement learning algorithms universally require multiple completions for each question, resulting in substantial computational costs. There is an urgent need to accelerate the training of reinforcement learning algorithms.
Inference Acceleration for Reasoning Models.
The enhancement of model inference capabilities is often accompanied by increased computational overhead and longer response times. Recent works have attempted to accelerate the inference process of reasoning models through efficient Chain of Thought (CoT) methods. TokenSkip [22 [https://arxiv.org/html/2503.22342v1#bib.bib22]] proposes a controllable CoT compression method that improves reasoning efficiency by selectively skipping less important tokens while preserving critical ones, thus achieving a balance between efficiency and accuracy.
Kang et al. [11 [https://arxiv.org/html/2503.22342v1#bib.bib11]] utilize a compressor to condense an original longer CoT into a shorter one while maintaining key information and interpretability. Although numerous works focus on inference acceleration, the acceleration of reasoning model training remains an underexplored area.
3 METHOD
3.1 PRELIMINARY
Group Relative Policy Optimization.
GRPO [6 [https://arxiv.org/html/2503.22342v1#bib.bib6]] foregoes the critic model that is typically the same size as the policy model and estimates the baseline from group scores instead.
Specifically, for each question $q$ sampled from the dataset distribution $P(Q)$, GRPO generates $G$ completions $\{o_{1},o_{2},\cdots,o_{G}\}$ using the old policy model $\pi_{\theta_{old}}$.
And then GRPO optimizes the policy model $\pi_{\theta}$ by maximizing the following objective:
$\displaystyle\mathcal{J}_{GRPO}(\theta)=\mathbb{E}_{q\sim P(Q),\{o_{i}\}_{i=1}%
^{G}\sim\pi_{\theta_{old}}(o|q)}\Bigg{\{}\frac{1}{G}\sum_{i=1}^{G}\frac{1}{|o_%
{i}|}\sum_{t=1}^{|o_{i}|}\Big{\{}\min\Big{[}\frac{\pi_{\theta}(o_{i,t}|q,o_{i,%
&lt;t})}{\pi_{\theta_{old}}(o_{i,t}|q,o_{i,&lt;t})}A_{i},$
$\displaystyle\text{clip}\big{(}\frac{\pi_{\theta}(o_{i,t}|q,o_{i,&lt;t})}{\pi_{%
\theta_{old}}(o_{i,t}|q,o_{i,&lt;t})},1-\epsilon,1+\epsilon\big{)}A_{i}\Big{]}-%
\beta\mathbb{D}_{KL}\left[\pi_{\theta}||\pi_{ref}\right]\Big{\}}\Bigg{\}}.$
(1)
where
$\mathbb{D}_{KL}\left[\pi_{\theta}||\pi_{ref}\right]=\frac{\pi_{ref}(o_{i,t}|q,%
o_{i,&lt;t})}{\pi_{\theta}(o_{i,t}|q,o_{i,&lt;t})}-\log\frac{\pi_{ref}(o_{i,t}|q,o_{%
i,&lt;t})}{\pi_{\theta}(o_{i,t}|q,o_{i,&lt;t})}-1.$
(2)
Here, $\epsilon$ and $\beta$ are hyperparameters.
$\pi_{\theta_{ref}}$ is the reference model which is usually the initial model before reinforcing learning.
And $A_{i}$ is the advantage computed using a group of rewards $\{r_{1},r_{2},\dots,r_{G}\}$ corresponding to the completions within each group:
$A_{i}=\frac{r_{i}-\mathrm{mean}(\{r_{1},r_{2},\dots,r_{G}\})}{\mathrm{std}(\{r%
_{1},r_{2},\dots,r_{G}\})}.$
(3)
Rule-based Reward Function. Instead of training an additional reward model for reward computation, GRPO employs a rule-based reward system that consists of two components:
$r_{i}=R_{format}(o_{i})+R_{accuracy}(o_{i}).$
(4)
Here, the format reward $R_{\text{format}}(o_{i})$ ensures that the output adheres to the expected structure, while the accuracy reward $R_{\text{accuracy}}(o_{i})$ prioritizes correctness with higher reward for accurate responses. 111The specific reward functions are provided in the Appendix A [https://arxiv.org/html/2503.22342v1#A1].
Analyzing Completion Impact on Policy Training.
From Eq. (1 [https://arxiv.org/html/2503.22342v1#S3.E1]), GRPO’s training overhead scales linearly with the number of completions sampled per question.
This arises from the necessity of calculating predicted probabilities for the policy, reference, and old policy models over all completions.
For instance, in DeepSeek-Math [17 [https://arxiv.org/html/2503.22342v1#bib.bib17]], using 64 completions requires 192 forward passes per question (64$\times$3), incurring significant computational costs. This raises two critical questions:
(1) How does the number of completions affect policy model accuracy? Does increasing completions always enhance performance?
(2) Do all completions in a group contribute equally to training?
To address the first question, we conduct an ablation study on GSM8K [3 [https://arxiv.org/html/2503.22342v1#bib.bib3]] using Qwen2.5-1.5B-Instruct [24 [https://arxiv.org/html/2503.22342v1#bib.bib24]].
Results in Figure 1 [https://arxiv.org/html/2503.22342v1#S3.F1] show that model accuracy improves with more completions but training time grows multiplicative.
This indicates diminishing returns on performance gains as training costs increase.
Crucially, reducing completions to cut costs risks degrading reasoning capabilities, making it impractical.
For the second question, we investigate whether completions contribute uniformly to training effectiveness. Our comprehensive analysis in Sec. 3.2 [https://arxiv.org/html/2503.22342v1#S3.SS2] reveals that completion contributions are highly variable, with some samples providing significantly more training signals than others. These findings motivate the development of strategies to identify and prioritize high-value completions, potentially improving training efficiency without compromising model performance.
Refer to caption [x1.png]
Refer to caption [x2.png]
Figure 1:
Completion number vs. (left) accuracy and (right) training time.
Experiments are conducted on GSM8K [3 [https://arxiv.org/html/2503.22342v1#bib.bib3]] using Qwen-1.5B-Instruct [24 [https://arxiv.org/html/2503.22342v1#bib.bib24]].
3.2 COMPLETION CONTRIBUTION ANALYSIS
To measure the contribution of each completion to the policy model training, we first compute the derivative of the policy objective function in Eq. (1 [https://arxiv.org/html/2503.22342v1#S3.E1]) with respect to the model parameters $\theta$ as:
$\displaystyle\nabla_{\theta}J_{GRPO}(\theta)=$
$\displaystyle\,\mathbb{E}_{\left[q\sim P(Q),\{o_{i}\}_{i=1}^{G}\sim\pi_{\theta%
_{old}}(O|q)\right]}\Bigg{\{}\frac{1}{G}\sum_{i=1}^{G}\frac{1}{\left|o_{i}%
\right|}\sum_{t=1}^{\left|o_{i}\right|}\Bigg{[}\nabla_{\theta}\left(\frac{\pi_%
{\theta}\left(o_{i,t}|q,o_{i,&lt;t}\right)}{\pi_{\theta_{old}}\left(o_{i,t}|q,o_{%
i,&lt;t}\right)}A_{i}\right)$
(5)
$\displaystyle\quad\quad\quad\quad-\beta\left(\nabla_{\theta}\frac{\pi_{ref}%
\left(o_{i,t}|q,o_{i,&lt;t}\right)}{\pi_{\theta}\left(o_{i,t}|q,o_{i,&lt;t}\right)}-%
\nabla_{\theta}\log\frac{\pi_{ref}\left(o_{i,t}|q,o_{i,&lt;t}\right)}{\pi_{\theta%
}\left(o_{i,t}|q,o_{i,&lt;t}\right)}\right)\Bigg{]}\Bigg{\}}$
$\displaystyle=$
$\displaystyle\mathbb{E}_{\left[q\sim P(Q),\{o_{i}\}_{i=1}^{G}\sim\pi_{\theta_{%
old}}(O|q)\right]}\Bigg{\{}\frac{1}{G}\sum_{i=1}^{G}\frac{1}{\left|o_{i}\right%
|}\sum_{t=1}^{\left|o_{i}\right|}\Bigg{[}\frac{\nabla_{\theta}\pi_{\theta}%
\left(o_{i,t}|q,o_{i,&lt;t}\right)}{\pi_{\theta_{old}\left(o_{i,t}|q,o_{i,&lt;t}%
\right)}}A_{i}$
$\displaystyle+\beta\left(\frac{\pi_{ref}\left(o_{i,t}|q,o_{i,&lt;t}\right)\nabla_%
{\theta}\pi_{\theta}\left(o_{i,t}|q,o_{i,&lt;t}\right)}{\pi_{\theta}^{2}\left(o_{%
i,t}|q,o_{i,&lt;t}\right)}-\frac{\nabla_{\theta}\pi_{\theta}\left(o_{i,t}|q,o_{i,%
&lt;t}\right)}{\pi_{\theta}\left(o_{i,t}|q,o_{i,&lt;t}\right)}\right)\Bigg{]}\Bigg{\}}$
$\displaystyle=$
$\displaystyle\mathbb{E}_{\left[q\sim P(Q),\{o_{i}\}_{i=1}^{G}\sim\pi_{\theta_{%
old}}(O|q)\right]}\Bigg{\{}\frac{1}{G}\sum_{i=1}^{G}\frac{1}{\left|o_{i}\right%
|}\sum_{t=1}^{\left|o_{i}\right|}\Bigg{[}\frac{\pi_{\theta}\left(o_{i,t}|q,o_{%
i,&lt;t}\right)}{\pi_{\theta_{old}}\left(o_{i,t}|q,o_{i,&lt;t}\right)}A_{i}$
$\displaystyle\quad\quad\quad\quad\quad\quad\quad+\beta\left(\frac{\pi_{ref}%
\left(o_{i,t}|q,o_{i,&lt;t}\right)}{\pi_{\theta}\left(o_{i,t}|q,o_{i,&lt;t}\right)}-%
1\right)\Bigg{]}\frac{\nabla_{\theta}\pi_{\theta}\left(o_{i,t}|q,o_{i,&lt;t}%
\right)}{\pi_{\theta}\left(o_{i,t}|q,o_{i,&lt;t}\right)}\Bigg{\}}$
$\displaystyle=$
$\displaystyle\mathbb{E}_{\left[q\sim P(Q),\{o_{i}\}_{i=1}^{G}\sim\pi_{\theta_{%
old}}(O|q)\right]}\Bigg{\{}\frac{1}{G}\sum_{i=1}^{G}\frac{1}{\left|o_{i}\right%
|}\sum_{t=1}^{\left|o_{i}\right|}\Bigg{[}\underbrace{\frac{\pi_{\theta}\left(o%
_{i,t}|q,o_{i,&lt;t}\right)}{\pi_{\theta_{old}}\left(o_{i,t}|q,o_{i,&lt;t}\right)}A_%
{i}}_{\textit{Advantage-weighted probability ratio}}$
$\displaystyle\quad\quad\quad\quad\quad+\underbrace{\beta\left(\frac{\pi_{ref}%
\left(o_{i,t}|q,o_{i,&lt;t}\right)}{\pi_{\theta}\left(o_{i,t}|q,o_{i,&lt;t}\right)}-%
1\right)}_{\textit{KL divergence constraint}}\Bigg{]}\underbrace{\nabla_{%
\theta}\log\pi_{\theta}\left(o_{i,t}|q,o_{i,&lt;t}\right)}_{\textit{Policy model %
gradient}}\Bigg{\}}.$
We analyze the derivative components stressed in Eq. (5 [https://arxiv.org/html/2503.22342v1#S3.E5]).
(1) Advantage-weighted probability ratio term $\frac{\pi_{\theta}\left(o_{i,t}|q,o_{i,&lt;t}\right)}{\pi_{\theta_{old}}\left(o_{%
i,t}|q,o_{i,&lt;t}\right)}A_{i}$. It directly ties the contribution of each completion to its advantage. This term incentivizes the policy to prioritize actions with higher rewards, as the advantage function quantifies how much a given action improves expected returns relative to the baseline. By amplifying high-advantage completions and suppressing low-advantage ones, this term guides policy optimization toward reward-aligned reasoning patterns.
(2) KL divergence constraint term $\beta\left(\frac{\pi_{ref}\left(o_{i,t}|q,o_{i,&lt;t}\right)}{\pi_{\theta}\left(o%
_{i,t}|q,o_{i,&lt;t}\right)}-1\right)$. It enforces stability by penalizing deviations from the reference model $\pi_{ref}$. However, this constraint is not inherently designed to shape the policy’s reasoning patterns but rather ensures smooth updates during training.
(3) Policy model gradient term $\nabla_{\theta}\log\pi_{\theta}\left(o_{i,t}|q,o_{i,&lt;t}\right)$. This term represents the gradient of the log-probability of the policy’s predicted action with respect to the model parameters $\theta$.
Recent work by Hu et al. [8 [https://arxiv.org/html/2503.22342v1#bib.bib8]] demonstrates that removing the KL divergence constraint does not impair the trained model’s reasoning ability, as the policy’s core reasoning patterns are primarily driven by the reward-aligned advantage term. Motivated by this insight, we approximate the policy objective’s derivative as:
$\displaystyle\nabla_{\theta}J_{GRPO}(\theta)$
$\displaystyle\approx\,\mathbb{E}_{\left[q\sim P(Q),\{o_{i}\}_{i=1}^{G}\sim\pi_%
{\theta_{old}}(O|q)\right]}$
(6)
$\displaystyle\Bigg{\{}\frac{1}{G}\sum_{i=1}^{G}\frac{1}{\left|o_{i}\right|}%
\sum_{t=1}^{\left|o_{i}\right|}\Big{[}\underbrace{\frac{\pi_{\theta}\left(o_{i%
,t}|q,o_{i,&lt;t}\right)}{\pi_{\theta_{old}}\left(o_{i,t}|q,o_{i,&lt;t}\right)}}_{%
\begin{subarray}{c}\textit{Probability ratio}\\
\textit{(Post-forward)}\end{subarray}}\cdot\underbrace{A_{i}}_{\begin{subarray%
}{c}\textit{Advantage}\\
\textit{(Prior-forward)}\end{subarray}}\Big{]}\underbrace{\nabla_{\theta}\log%
\pi_{\theta}\left(o_{i,t}|q,o_{i,&lt;t}\right)}_{\begin{subarray}{c}\textit{%
Policy model gradient}\\
\textit{(Post-forward)}\end{subarray}}\Bigg{\}},$
focusing on the reward-driven learning signal while decoupling the KL regularization constraint.
To better understand this formulation, we decompose the advantage-weighted probability ratio term into the Probability ratio term and the Advantage term. For a completion that significantly contributes to the policy update, all the new three components in Eq. (6 [https://arxiv.org/html/2503.22342v1#S3.E6]) must be non-negligible. A near-zero or zero value in any of these components would render the overall contribution minimal or nonexistent.
From a computational timing perspective, these components can be categorized as:
(1) the probability ratio and policy model gradient are post-forward information, meaning they can only be computed after the policy’s forward computation.
(2) The advantage term, however, represents prior-forward information that can be calculated before the policy’s forward computation.
Given our objective to accelerate GRPO training, we focus on leveraging this prior-forward information.
By evaluating the advantage term before the forward computation, we can make an informed decision about whether to process a completion through the policy model.
Specifically, if the absolute value of the advantage for a completion is so small or insignificant that it can be practically treated as if it were zero without causing any significant difference in the outcome, we prune that completion from the batch.
This selective processing ensures that only completions with high absolute advantage proceed to the forward computation and gradient update stages.
3.3 COMPLETIONS PRUNING POLICY OPTIMIZATION
As shown in Figure 2 [https://arxiv.org/html/2503.22342v1#S3.F2], we propose the Completions Pruning Policy Optimization (CPPO) algorithm to accelerate the training process of group relative policy optimization. Compared to GRPO’s optimization objective in Eq. (1 [https://arxiv.org/html/2503.22342v1#S3.E1]), our CPPO introduces a selective condition that only includes completions exhibiting a sufficiently high advantage. The CPPO objective is formulated as follows:
$\displaystyle\mathcal{J}_{CPPO}(\theta)=$
$\displaystyle\,\mathbb{E}_{q\sim P(Q),\{o_{i}\}_{i=1}^{G}\sim\pi_{\theta_{old}%
}(o|q)}\Bigg{\{}\frac{1}{G}\sum_{i=1}^{G}\frac{1}{|o_{i}|}\sum_{t=1}^{|o_{i}|}%
\Big{\{}\min\Big{[}\frac{\pi_{\theta}(o_{i,t}|q,o_{i,&lt;t})}{\pi_{\theta_{old}}(%
o_{i,t}|q,o_{i,&lt;t})}A_{i},$
(7)
$\displaystyle\quad\quad\quad\text{clip}\big{(}\frac{\pi_{\theta}(o_{i,t}|q,o_{%
i,&lt;t})}{\pi_{\theta_{old}}(o_{i,t}|q,o_{i,&lt;t})},1-\epsilon,1+\epsilon\big{)}A_%
{i}\Big{]}-\beta\mathbb{D}_{KL}\left[\pi_{\theta}||\pi_{ref}\right]\Big{\}}%
\Bigg{\}},$
$\displaystyle\qquad\qquad\qquad\qquad\quad s.t.\quad|A_{i}|\geq\gamma,$
where $\gamma$ is a predefined threshold that ensures only completions with an absolute advantage above $\gamma$ are retained in the gradient update.
It should be noted that when the ratio $\frac{\pi_{\theta}(o_{i,t}|q,o_{i,&lt;t})}{\pi_{\theta_{old}}(o_{i,t}|q,o_{i,&lt;t})%
}&lt;1-\epsilon$ and $A_{i}&lt;0$, or, $\frac{\pi_{\theta}(o_{i,t}|q,o_{i,&lt;t})}{\pi_{\theta_{old}}(o_{i,t}|q,o_{i,&lt;t})%
}&lt;1+\epsilon$ and $A_{i}&gt;0$, the clip function is activated.
This action effectively nullifies the policy model gradient term in Eq. (6 [https://arxiv.org/html/2503.22342v1#S3.E6]), equivalent to pruning all completions.
Unifying Single-/Multiple-GPU(s) Settings. In a multi-GPUs training scenario, we observe that the number of completions with significant advantages varies across devices. In such cases, the overall training efficiency is bottlenecked by the device processing the largest number of completions—a phenomenon referred to as the bucket effect. To mitigate this, for each GPU, we retain only the $k$ completions with the largest absolute advantage for each question, where
$k=\lfloor G\times(1-P)\rfloor,$
(8)
where $P\in(0,1]$ denoting the pruning rate. The modified CPPO under this strategy is:
$\displaystyle\mathcal{J}_{CPPO}(\theta)=$
$\displaystyle\,\mathbb{E}_{q\sim P(Q),\{o_{i}\}_{i=1}^{G}\sim\pi_{\theta_{old}%
}(o|q)}\Bigg{\{}\frac{1}{k}\sum_{i\in\mathcal{I}}\frac{1}{|o_{i}|}\sum_{t=1}^{%
|o_{i}|}\Big{\{}\min\Big{[}\frac{\pi_{\theta}(o_{i,t}|q,o_{i,&lt;t})}{\pi_{\theta%
_{old}}(o_{i,t}|q,o_{i,&lt;t})}A_{i},$
(9)
$\displaystyle\quad\quad\quad\text{clip}\big{(}\frac{\pi_{\theta}(o_{i,t}|q,o_{%
i,&lt;t})}{\pi_{\theta_{old}}(o_{i,t}|q,o_{i,&lt;t})},1-\epsilon,1+\epsilon\big{)}A_%
{i}\Big{]}-\beta\mathbb{D}_{KL}\left[\pi_{\theta}||\pi_{ref}\right]\Big{\}}%
\Bigg{\}},$
where the summation is taken only over the index set $\mathcal{I}$ corresponding to the $k$ completions with the highest absolute advantage values, i.e.,
$\mathcal{I}=\{i\in\{1,...,G\}\;\big{|}\;|A_{i}|\text{ is among the top $k$ %
values}\}.$
(10)
In Sec. 4.2.2 [https://arxiv.org/html/2503.22342v1#S4.SS2.SSS2], we analyze that completions with high absolute advantage values, either having a correct format and correct answer or an incorrect format and incorrect answer, provide the clearest training signals.
Partial correct completions with small absolute advantages contribute minimally or may mislead the policy model.
Removing these completions from the training process can enhance training efficiency without compromising model performance.
Refer to caption [x3.png]
Figure 2: Overview of Completion Pruning Policy Optimization (CPPO). After obtaining the advantages, only completions with high absolute advantage are retained for the forward of policy model, reference model, and old policy model.
$\mathbb{D}_{R}$ is the probability ratio of policy to old policy models.
The pipeline of the CPPO algorithm is as follows:
(1) The old policy model samples a group of completions for each question.
(2) The reward function computes the reward for each completion via Eq. (4 [https://arxiv.org/html/2503.22342v1#S3.E4]).
(3) The relative advantage of each completion is calculated according to Eq. (3 [https://arxiv.org/html/2503.22342v1#S3.E3]).
(4) CPPO retains $k$ completions with highest absolute advantages.
(5) The policy model is updated based on the selected completions.
The key distinction between CPPO and GRPO is that CPPO does not use all completions for the forward computation of the policy model, reference model, and old policy model. Instead, by retaining only those with high absolute advantages for the gradient update, CPPO significantly reduces the computational overhead during forward passes, thereby accelerating the training process.
Refer to caption [x4.png]
Figure 3: Illustration of dynamic completion allocation for parallel processing. After pruning completions, completion allocation incorporates important completions from new questions. The $o_{m}^{n}$ represents the $m$-th completion of the $n$-th question.
3.4 PARALLEL PROCESSING THROUGH DYNAMIC COMPLETION ALLOCATION
In this section, we introduce a novel dynamic completion allocation strategy to further optimize the training efficiency of CPPO. Conventional approaches, such as those employed in GRPO, face inherent limitations due to GPU memory constraints. Specifically, a single device can process a maximum of $B$ questions per batch, with each question generating $G$ candidate completions. After pruning, the total number of retained completions per device reduces to $B\times k$, resulting in suboptimal GPU utilization and underleveraged parallel computing capabilities.
To address this inefficiency, we dynamically allocate pruned completions from additional questions into the device’s processing pipeline, as illustrated in Figure 3 [https://arxiv.org/html/2503.22342v1#S3.F3]. This strategy ensures that each device operates at full capacity by continuously populating its memory with high-quality completions derived from both the original and newly introduced questions. Critically, all newly incorporated completions undergo the same rigorous pruning process to maintain consistency and relevance.
The benefits of this approach are twofold. First, it maximizes GPU utilization by fully exploiting the device’s parallel computing potential. Second, it enables each device to process a larger number of questions per batch, thereby reducing the total number of training steps required to achieve convergence.
This dual optimization boosts training efficiency while maintaining training quality.
4 EXPERIMENTS
4.1 EXPERIMENTAL SETTINGS
Training Details. We implement CPPO based on the Open R1 framework [4 [https://arxiv.org/html/2503.22342v1#bib.bib4]], which uses vLLM inference library [12 [https://arxiv.org/html/2503.22342v1#bib.bib12]] to generate completions.
The Qwen2.5-1.5B-Instruct and Qwen2.5-7B-Instruct models are trained using two and four GPUs, respectively, each with 80GB of memory.
The $\epsilon$ and $\beta$ in Eq. (7 [https://arxiv.org/html/2503.22342v1#S3.E7]) are set to $0.2$ and $0.04$.
The batch size is set to 16, and the learning rate is set to $1e-6$.
The temperature of the policy model is set to 1 and the group size is set to 16.
The maximum length of each completion is set to 1024.
Evaluation Details. We evaluate the performance on multiple benchmarks with different difficulty, including Math [7 [https://arxiv.org/html/2503.22342v1#bib.bib7]], AIME2024 [14 [https://arxiv.org/html/2503.22342v1#bib.bib14]], AMC2023 [13 [https://arxiv.org/html/2503.22342v1#bib.bib13]], and GSM8K [3 [https://arxiv.org/html/2503.22342v1#bib.bib3]]. We use vLLM [12 [https://arxiv.org/html/2503.22342v1#bib.bib12]] to accelerate the evaluation process. The evaluation batch size is set to 10.
We use greedy decoding to generate completions for Math and GSM8K.
For AIME2024 and AMC2023, we set the temperature as $0.6$ and use $4$ completions for each question. We use Pass@1 accuracy as the evaluation metric.
4.2 MAIN RESULTS
We assess CPPO by training models of different scales on the GSM8K [3 [https://arxiv.org/html/2503.22342v1#bib.bib3]] and MATH [7 [https://arxiv.org/html/2503.22342v1#bib.bib7]] datasets.
GSM8K contains 8.5K grade-school math problems, while MATH includes 7.5K competition-level problems.
Given the reasoning capabilities of the base models, we use Qwen2.5-1.5B-Instruct for the relatively simpler GSM8K dataset and Qwen2.5-7B-Instruct for the more complex MATH dataset.
For evaluation, each model is tested on the corresponding dataset’s test subset.
Additionally, to assess the models’ out-of-distribution reasoning ability, we introduce AMC2023 [13 [https://arxiv.org/html/2503.22342v1#bib.bib13]] and AIME2024 [14 [https://arxiv.org/html/2503.22342v1#bib.bib14]] as test benchmarks.
Since AMC2023 and AIME2024 are too challenging for the Qwen2.5-1.5B-Instruct model, we evaluate only the Qwen2.5-7B-Instruct model on these benchmarks.
Table 1:
Comparison between GRPO and CPPO on GSM8K test subset. We train Qwen2.5-1.5B-Instruct on the GSM8K training subset, and the number of retained completions after pruning is denoted by $k=\lfloor G\times(1-P)\rfloor$.
Method
Group
Pruning
$k$
Accuracy
Training
Accelerate
Size ($G$)
Rate ($P$)
Time
Ratio
Qwen2.5-1.5B-Instruct
-
-
-
55.72%
-
-
GRPO
16
0.00%
16
77.05 %
23393s
$1.00\times$
CPPO
16
50.00%
8
77.67%
12930s
$1.81\times$
CPPO
16
75.00%
4
78.81%
7159s
$3.27\times$
CPPO
16
87.50%
2
80.41%
4781s
$4.89\times$
CPPO
16
93.75%
1
78.20%
2813s
$8.32\times$
4.2.1 PERFORMANCE COMPARISON
Training on GSM8K.
As shown in Table 1 [https://arxiv.org/html/2503.22342v1#S4.T1], CPPO demonstrates clear advantages over GRPO in both accuracy and acceleration ratio.
Notably, CPPO achieves comparable or even higher accuracy than GRPO across various pruning rates. At a pruning rate of $87.50\%$, CPPO attains an accuracy of $80.41\%$, surpassing GRPO’s $77.05\%$ by $3.36\%$.
For efficiency, CPPO greatly accelerates training. At a pruning rate of $93.75\%$, it achieves an acceleration ratio of $8.32\times$.
The speedup stems from the completions pruning and the completions allocation.
Completions pruning reduces computational overhead by discarding less important completions, while the completions allocation strategy maximizes the use of freed memory and leverages the GPU’s parallel processing capabilities.
As a result, CPPO processes more questions per batch and reduces the total number of training steps required.
These results demonstrate that CPPO not only maintains or improves accuracy but also significantly enhances training efficiency, making it a practical and effective solution for large-scale reasoning model training.
Table 2:
Comparison of GRPO and CPPO on the MATH test subset, as well as on out-of-distribution benchmarks AMC 2023 and AIME 2024.
We train Qwen2.5-7B-Instruct on the MATH training dataset, and the number of retained completions after pruning is denoted by $k=\lfloor G\times(1-P)\rfloor$.
Method
Group
Pruning
$k$
Accuracy
Time
Accelerate
AMC
AIME
Size
Rate
Ratio
2023
2024
Qwen2.5-7B-Instruct
-
-
-
55.20%
-
-
25.62%
5.00%
GRPO
16
0.00%
16
75.20%
33902s
$1.00\times$
46.88%
5.83%
CPPO
16
50.00%
8
75.20%
20550s
$1.65\times$
53.12%
10.00%
CPPO
16
75.00%
4
77.20%
12959s
$2.62\times$
49.38%
6.67%
CPPO
16
87.50%
2
75.20%
9657s
$3.51\times$
46.25%
8.33%
CPPO
16
93.75%
1
72.80%
8375s
$4.05\times$
45.00%
5.83%
Training on MATH. In Table 2 [https://arxiv.org/html/2503.22342v1#S4.T2], CPPO can well scale to larger models, achieving up to $3.51\times$ acceleration on the MATH without sacrificing accuracy.
For instance, at a pruning rate of 87.5%, CPPO maintains the same accuracy as GRPO (75.20%) while reducing training time by 3.51$\times$.
Furthermore, evaluation on the AMC2023 and AIME2024 benchmarks confirms that CPPO, despite training only on high absolute advantage completions, preserves the model’s generalization ability on out-of-distribution tasks. Thus, CPPO not only matches or even surpasses GRPO in enhancing reasoning capabilities but also well reduces training time, making it a more efficient alternative.
Stability and Convergence.
We plot the reward curves in Figure 4 [https://arxiv.org/html/2503.22342v1#S4.F4] during training on both GSM8K and MATH datasets. Overall, the reward curves provide evidence that CPPO preserves GRPO’s training stability while improving convergence speed.
The results show that the reward curves of CPPO do not crash, or experience drastic fluctuations, which is crucial for stable training.
These results suggest CPPO’s robust and stable training stability.
Moreover, the reward curves of CPPO show a clear upward trend, reaching higher reward values more quickly than GRPO.
This faster increase in reward values indicates that CPPO converges more rapidly.
Refer to caption [x5.png]
Refer to caption [x6.png]
Figure 4: Reward comparison during training (1 epoch) between GRPO and CPPO. Left: GSM8K dataset. Right: Math dataset.
4.2.2 AN IN-DEPTH ANALYSIS OF CPPO’S HIGHER ACCURACY
Results on Sec. 4.2.1 [https://arxiv.org/html/2503.22342v1#S4.SS2.SSS1] indicate that CPPO achieves better performance at high pruning rates.
To rule out the possibility that this improvement is merely due to the increased number of questions processed per training step, which is enabled by the completion allocation strategy, we compare CPPO with GRPO under the same number of questions per training step, as shown in Figure 5 [https://arxiv.org/html/2503.22342v1#S4.F5].
The key difference is that CPPO first generates a group of completions and retains only the top $k$ completions for gradient update, whereas GRPO directly generates $k$ completions for update.
Despite this, CPPO consistently outperforms GRPO, demonstrating that its accuracy gains stem not from processing more questions per step but from the higher quality of retained completions.
The quality of completions plays a crucial role in training.
CPPO selectively retains high absolute advantage completions from a larger pool, whereas GRPO updates the model with directly generated completions, which may vary in quality.
This aligns with our completion contribution analysis in Sec. 3.2 [https://arxiv.org/html/2503.22342v1#S3.SS2], which highlights that completions with high absolute advantages contribute more effectively to training.
In GRPO with a group size of 16, both high and low absolute advantage completions are used for training.
In contrast, CPPO with an 87.50% pruning rate trains exclusively on high-advantage completions—yet still achieves superior performance.
To better understand this, we categorize completions into four types (illustrated in Table 3 [https://arxiv.org/html/2503.22342v1#S4.T3]):
(1) Correct format and correct answer — Guides the model to generate accurate completions.
(2) Incorrect format and incorrect answer — Helps the model avoid incorrect completions.
(3) Correct format and incorrect answer — May mislead the model into generating partially correct responses.
(4) Incorrect format but correct answer — Similarly, may introduce noise in learning.
The first two types are high-quality completions that provide clear training signals. The latter two types, however, are low-quality completions, as their small positive advantage values can mislead the model and introduce noise.
Unlike GRPO, which trains on all completions indiscriminately, CPPO filters out these low-quality completions through completions pruning, leading to more efficient learning and better overall performance.
As more low-quality completions are removed (pruning rate: 0.00% $\rightarrow$ 87.50%), performance improves, as shown in Table 1 [https://arxiv.org/html/2503.22342v1#S4.T1] and Table 2 [https://arxiv.org/html/2503.22342v1#S4.T2].
However, an excessively high pruning rate can also discard high-quality completions, reducing training effectiveness.
This is evident in CPPO’s performance decline at a 93.75% pruning rate, as shown in Table 1 [https://arxiv.org/html/2503.22342v1#S4.T1] and Table 2 [https://arxiv.org/html/2503.22342v1#S4.T2].
Refer to caption [x7.png]
Refer to caption [x8.png]
Figure 5:
Evaluation accuracy comparison.
Left: Qwen2.5-1.5b-Instruct on GSM8K test subset.
Right: Qwen2.5-7b-Instruct on MATH test subset.
Here, $k$ denotes retained completion quantity by CPPO (or generated by GRPO), and $b$ represents questions per training step.
Table 3: Illustration of completions from Qwen2.5-1.5B-Instruct [24 [https://arxiv.org/html/2503.22342v1#bib.bib24]], using a question from GSM8K [3 [https://arxiv.org/html/2503.22342v1#bib.bib3]].
The “✓” (“✗”) denotes correct (incorrect) formats or answers.
Question:
The gauge on a water tank shows that the tank is 1/3 full of water. To fill the tank, 16 gallons of water are added. How many gallons of water does the tank hold when full?
Completion Type 1:
<think> Let’s denote the full capacity of the tank by C gallons. The gauge shows that the tank is 1/3 full which means it is 1/3 of C gallons. If we add 16 gallons to the tank, it will be filled completely. So, C + 16 = C * 2/3 (since 1/3 + 1/3 + 1/3 is equal to 1, and 1/f + 1/f + 1/f is equal to 3/f from geometry). Simplifying this equation, we get C = 16 * 3/2 = 24 gallons. </think>
<answer> The tank holds 24 gallons when full. </answer>
Format Reward: ✓
Accuracy Reward: ✓
Advantage: 2.01
Completion Type 2:
<think> Let’s denote the total capacity of the tank as $C$ gallons. According to the problem, when the tank is 1/3 full, it contains 1/3 $C$ gallons of water. When 16 gallons are added to this, the total becomes full, which means $C=16+\frac{1}{3}C$. This equation helps us find the total capacity $C$. </think>
Format Reward: ✗
Accuracy Reward: ✗
Advantage: -1.26
Completion Type 3:
<think>We can set up an equation based on the information in the problem. Let x be the total number of gallons the tank holds when full. </think>
<answer> x </answer>
Format Reward: ✓
Accuracy Reward: ✗
Advantage: 0.04
Completion Type 4:
<think> Let’s reason through this step by step: </think>
1. We know that the tank was 1/3 full.
2. To find out the total capacity of the tank when it’s full, we need to multiply the amount already in the tank by 3 (since 1 full tank = 3/3 full tanks).
3. We also know that adding 16 gallons filled the tank to its full capacity.
4. So, if 16 gallons equals the difference between the full tank and the current amount, we can use this information to find the current amount in gallons.
5. If 16 gallons are added to 4/3 of the total capacity to get the full capacity, then the current amount must be 3/4 of the full capacity.
6. Let’s call the total capacity of the tank C (it’s a whole number, in gallons).
So, the current amount in the tank = (3/4)C
7. Rearranging 3/4 to look like 3C/4, and knowing that (3C/4) = C - (1/3C) = 2/3C, and we know that 2/3C = 16,
8. Therefore 2/3 * C = 16,
9. So C must be 16 * 3/2,
10. Which equals 24 gallons.
<answer> The tank holds 24 gallons when full. </answer>
Format Reward: ✗
Accuracy Reward: ✓
Advantage: 0.69
Table 4: Evaluation of CPPO with different pruning metrics on GSM8K. “Largest”/“Smallest” prune completions with the highest/lowest absolute advantages, while “Largest*”/“Smallest*” use raw advantage values. “Random” denotes random pruning.
Pruning Meric
Group Size
Pruning Rate
Accuracy
Qwen2.5-1.5B-Instruct
-
-
55.72%
Largest*
16
50.0%
73.32%
Smallest*
16
50.0%
76.83%
Largest
16
50.0%
74.23%
Random
16
50.0%
76.98%
Smallest
16
50.0%
77.67%
Table 5: Ablation Study on the key components of CPPO. Experiments are conducted on Math [7 [https://arxiv.org/html/2503.22342v1#bib.bib7]] using Qwen2.5-7B-Instruct [24 [https://arxiv.org/html/2503.22342v1#bib.bib24]].
Method
Group
Pruning
Accuracy
Time
Accelerate
Size
Rate
Ratio
GRPO
16
0.0%
75.20%
33902s
$1.00\times$
+ Completion Pruning
16
50.0%
75.80%
27547s
$1.23\times$
+ Completion Allocation
16
50.0%
75.20%
20550s
$1.65\times$
4.3 ABLATION STUDY
In this section, we first conduct an ablation study on the design of the pruning metric and then evaluate the effectiveness of the two key modules: completion pruning and completion allocation.
Ablation Study on Pruning Metrics.
In Sec. 3.2 [https://arxiv.org/html/2503.22342v1#S3.SS2], we analyze the contribution of each completion using the derivative of the policy objective function.
The analysis reveals that a completion’s impact on policy model training is tied to the absolute value of its advantage—higher absolute value provides stronger training signals.
Based on this insight, we adopt the absolute advantage value as the pruning metric, removing completions with the lowest absolute advantages.
As shown in Table 4 [https://arxiv.org/html/2503.22342v1#S4.T4], “Smallest” achieves the best performance, while “Largest” performs the worst, with “Random” falling in between.
Additionally, “Smallest*”and “Largest*”, which prune completions based on raw advantage values rather than absolute values, perform worse than “Smallest”, confirming that absolute advantage values are a more effective pruning metric.
The results align with our analysis in Sec. 3.2 [https://arxiv.org/html/2503.22342v1#S3.SS2] and further validate the effectiveness of pruning based on absolute advantage values.
Ablation Study on Key Modules.
As shown in Table 5 [https://arxiv.org/html/2503.22342v1#S4.T5], by discarding unimportant completions, the completion pruning module improves the training efficiency by $1.23\times$.
By fully leveraging the benefits brought by completion pruning and the GPU parallel computing capability, the completion allocation strategy further improves the training efficiency to $1.65\times$.
4.4 CASE STUDY
As shown in Table 6 [https://arxiv.org/html/2503.22342v1#S4.T6], we provide a case study to illustrate the differences between completions generated by GRPO and CPPO.
We find that CPPO can maintain the reasoning ability of the trained model.
Note that CPPO significantly reduces the training time, achieving at most $8.32\times$ acceleration on GSM8K datasets.
More case studies are provided in the Appendix C [https://arxiv.org/html/2503.22342v1#A3].
Table 6:
The comparisons of completions generated by GRPO and CPPO.
The question is sampled from the GSM8K [3 [https://arxiv.org/html/2503.22342v1#bib.bib3]] dataset.
We use Qwen2.5-1.5B-Instruct [24 [https://arxiv.org/html/2503.22342v1#bib.bib24]] model with different training methods to generate completions.
The “✓” and “✗” denote a correct and wrong response, respectively.
Question:
Ian has a board that is 40 feet long. He decides to make a cut so he can have two pieces. The longer piece is 4 times longer than the shorter piece. How long is the longer piece?
GRPO Response ✓:
<think> Let’s denote the length of the shorter piece as $L$. According to the problem, the longer piece is 4 times longer than the shorter piece, so the longer piece’s length is $4L$. The total length of the board is the sum of the lengths of the two pieces, which is 40 feet. Therefore, we have the equation: $L+4L=40$. Simplifying this, we get $5L=40$. Solving for $L$, we get $L=8$ feet. Since the longer piece is 4 times longer than the shorter piece, the longer piece’s length is $4\times 8=32$ feet. </think>
<answer> 32 </answer>
CPPO (50.00%) Response ✓:
<think> Let’s denote the length of the shorter piece as $x$. Then the longer piece would be $4x$. The total length of the board is the sum of the lengths of the two pieces, so $x+4x=40$. This simplifies to $5x=40$. Solving for $x$, we get $x=8$. Therefore, the longer piece is $4x=4\times 8=32$ feet. </think>
<answer> 32 </answer>
CPPO (75.00%) Response ✓:
<think> Let’s denote the length of the shorter piece as $x$. Then, the length of the longer piece is $4x$. The total length of the board is the sum of the lengths of the two pieces, so we have the equation: $x+4x=40$. This simplifies to $5x=40$. Solving for $x$, we get $x=8$. Therefore, the length of the longer piece is $4x=4\times 8=32$ feet. </think>
<answer> 32 </answer>
CPPO (87.5%) Response ✓:
<think> Let’s denote the length of the shorter piece as $x$ feet. According to the problem, the longer piece is 4 times longer than the shorter piece, so the longer piece is $4x$ feet. The total length of the board is the sum of the lengths of the two pieces, which is 40 feet. Therefore, we can write the equation: $x+4x=40$. Simplifying the left side, we get $5x=40$. To find $x$, we divide both sides by 5, so $x=8$. This means the shorter piece is 8 feet long. The longer piece is 4 times the length of the shorter piece, so it is $4\times 8=32$ feet. </think>
<answer> 32 </answer>
GRPO (93.75%) Response ✓:
<think> Let’s denote the length of the shorter piece as $x$. According to the problem, the longer piece is 4 times longer than the shorter piece, so the length of the longer piece is $4x$. The total length of the board is the sum of the lengths of the two pieces, so we can write the equation: $x+4x=40$. Combining like terms, we get $5x=40$. To find $x$, we divide both sides by 5: $x=8$. So, the length of the shorter piece is 8 feet. The longer piece is 4 times the length of the shorter piece, so it is $4\times 8=32$ feet. Therefore, the longer piece is 32 feet long. </think>
<answer> 32 </answer>
5 LIMITATIONS AND FUTURE WORK
Due to limited GPU resources, we only evaluate the effectiveness of CPPO on relatively small-scale models (less than 7B) and datasets. Specifically, our current assessment of CPPO’s effectiveness focuses on math datasets, including Math and GSM8K.
In the future, we plan to:
(1) valuate CPPO on larger models and diverse datasets.
(2) explore CPPO in multimodal large language model reinforcement learning.
(3) test CPPO with other policy optimization algorithms, such as PPO [16 [https://arxiv.org/html/2503.22342v1#bib.bib16]] and REINFORCE++ [23 [https://arxiv.org/html/2503.22342v1#bib.bib23]].
We hope that research groups and industries with substantial GPU resources can join us in experimentally verifying CPPO on larger models and more extensive datasets.
Collaborative efforts will not only advance the field but also reduce AI costs, ultimately benefiting human society.
6 CONCLUSION
In this paper, we have introduced Completion Pruning Policy Optimization (CPPO) to enhance the training efficiency of reasoning model training with the GRPO algorithm.
By selectively pruning completions based on their relative advantages, CPPO reduces computational overhead without compromising model performance. Additionally, our dynamic completion allocation strategy fully leverages the benefits of completion pruning and GPU parallelism, further boosting training speed.
The results demonstrate that CPPO achieves up to $8.32\times$ speedup on GSM8K and $3.51\times$ on Math while preserving or even enhancing the accuracy compared to the original GRPO.
These findings highlight CPPO as a practical solution for optimizing reasoning model training at a lower cost.
7 ACKNOWLEDGMENTS
This work was supported by the National Science Fund for Distinguished Young Scholars (No.62025603), the National Natural Science Foundation of China (No. U21B2037, No. U22B2051, No. U23A20383, No. 62176222, No. 62176223, No. 62176226, No. 62072386, No. 62072387, No. 62072389, No. 62002305 and No. 62272401), and the Natural Science Foundation of Fujian Province of China (No. 2021J06003, No.2022J06001).
APPENDIX
APPENDIX A SPECIFIC DESIGN OF THE REWARD FUNCTION
In this paper, we define these reward components for GSM8K datasets as follows:
$\displaystyle R_{\text{format}}(o_{i})$
$\displaystyle=\begin{cases}1,&amp;\text{if }o_{i}\text{ follows the correct format%
},\\
0,&amp;\text{otherwise}.\end{cases}$
(11)
$\displaystyle R_{\text{accuracy}}(o_{i})$
$\displaystyle=\begin{cases}2,&amp;\text{if }o_{i}\text{ directly matchs the %
correct answer},\\
1.5&amp;\text{if }o_{i}\text{ matchs the correct answer after regular parsing},\\
0,&amp;\text{otherwise}.\end{cases}$
And the reward components for Math datasets are defined as follows:
$\displaystyle R_{\text{format}}(o_{i})$
$\displaystyle=\begin{cases}1,&amp;\text{if }o_{i}\text{ follows the correct format%
},\\
0,&amp;\text{otherwise}.\end{cases}$
(12)
$\displaystyle R_{\text{accuracy}}(o_{i})$
$\displaystyle=\begin{cases}2,&amp;\text{if }o_{i}\text{ directly matchs the %
correct answer},\\
0,&amp;\text{otherwise}.\end{cases}$
APPENDIX B TEMPLATES OF COMPLETION PRUNING POLICY OPTIMIZATION
In this section, we provide the templates of our Completion Pruning Policy Optimization for GSM8K [3 [https://arxiv.org/html/2503.22342v1#bib.bib3]] and Math [7 [https://arxiv.org/html/2503.22342v1#bib.bib7]] datasets. The templates are shown in Table 7 [https://arxiv.org/html/2503.22342v1#A2.T7] and Table 8 [https://arxiv.org/html/2503.22342v1#A2.T8].
Table 7: The templates of our CPPO for GSM8K [3 [https://arxiv.org/html/2503.22342v1#bib.bib3]] dataset.
Question will be replaced by the question in the dataset.
A conversation between User and Assistant. The user asks a question, and the Assistant solves it. The assistant first thinks about the reasoning process in the mind and then provides the user with the answer. The reasoning process and answer are enclosed within <think> </think> and <answer> </answer> tags, respectively, i.e., <think> reasoning process here </think>\n<answer> answer here </answer>.
User: Question.
Assistant:
Table 8: The templates of our CPPO for Math [7 [https://arxiv.org/html/2503.22342v1#bib.bib7]] dataset. Question will be replaced by the question in the dataset.
A conversation between User and Assistant. The user asks a question, and the Assistant solves it. The assistant first thinks about the reasoning process in the mind and then provides the user with the answer. And the answer should be of the following format: “Therefore, the final answer is: \boxed{ANSWER}. I hope it is correct.” (without quotes) where ANSWER is just the final number or expression that solves the problem. The reasoning process and answer are enclosed within <think> </think> and <answer> </answer> tags, respectively, i.e., <think> reasoning process here </think>\n<answer> answer here </answer>.
User: Question.
Assistant:
APPENDIX C MORE CASE STUDIES
As shown in Table 9 [https://arxiv.org/html/2503.22342v1#A3.T9], there are some questions where GRPO fails to provide the correct answer, while CPPO with a high pruning rate succeeds in providing the correct answer.
This indicates that CPPO can maintain or even improve the reasoning ability of the trained model by pruning completions with low absolute advantages during training.
Table 9: The comparisons of completions generated by GRPO and CPPO.
The question is from the GSM8K [3 [https://arxiv.org/html/2503.22342v1#bib.bib3]] dataset and generated by Qwen2.5-1.5B-Instruct [24 [https://arxiv.org/html/2503.22342v1#bib.bib24]].
The “✓” denotes a correct response while the “✗” indicates a wrong response.
Question:
Lee used to be able to run the 400-meter hurdles two seconds faster than Gerald would run the 400-meter hurdles. But Gerald changed his diet, which improved his speed by 10%. If Lee runs the 400-meter hurdles in 38 seconds, how fast can Gerald, with his improved diet, run the 400-meter hurdles, in seconds?
GRPO Response ✗:
<think> Let’s break down the problem step by step. Lee used to be able to run the 400-meter hurdles two seconds faster than Gerald. This means that if Lee runs in 38 seconds, Gerald would run in 38 + 2 = 40 seconds. Gerald’s diet improved his speed by 10%, so we need to calculate his new speed. If Gerald’s original speed was $S$ seconds, his improved speed would be $S\times 1.1$. We know that Lee’s speed is 2 seconds faster than Gerald’s original speed, so we can set up the equation: $38=S-2$. Solving for $S$, we get $S=38+2=40$ seconds. Now, we can calculate Gerald’s improved speed: $40\times 1.1=44$ seconds. </think>
<answer> 44 </answer>
CPPO (50.00%) Response ✗:
<think> Let’s break down the problem step by step. First, we need to determine how long it used to take Lee to run the 400-meter hurdles. Since Lee used to be able to run it two seconds faster than Gerald, and Lee now takes 38 seconds, we can find out how long it used to take Lee to run the hurdles. Let’s denote the time it took Lee to run the hurdles before the diet change as $T$ seconds. Since Lee now takes 38 seconds and he used to be two seconds faster, we have $T-2=38$. Solving this equation, we get $T=40$ seconds. This means that before the diet change, Lee took 40 seconds to run the 400-meter hurdles.
Now, we need to find out how long it took Gerald to run the 400-meter hurdles before the diet change. Since Lee used to be able to run it two seconds faster than Gerald, we have $T_{\text{Gerald}}=40-2=38$ seconds. This means that before the diet change, Gerald took 38 seconds to run the 400-meter hurdles.
Now, we need to find out how fast Gerald can run the 400-meter hurdles with his improved diet. Since his diet improved his speed by 10%, we need to calculate 110% of his original speed. Let’s denote the original speed of Gerald as $S$ meters per second. Since he took 38 seconds to run the 400-meter hurdles before the diet change, we have $S=\frac{400}{38}$ meters per second. With his improved diet, his speed is now $1.1\times S$. Substituting the value of $S$, we get $1.1\times\frac{400}{38}=\frac{440}{38}$ meters per second. Simplifying this, we get $\frac{440}{38}=11.57$ meters per second. </think>
<answer> 11.57 </answer>
CPPO (75.00%) Response ✗:
<think> First, we need to determine how long it used to take Lee to run the 400-meter hurdles. Since Lee used to be able to run it two seconds faster than Gerald, and Lee runs it in 38 seconds, we can calculate the time it used to take Gerald to run it. The time it used to take Gerald is $38+2=40$ seconds. Next, we need to calculate how fast Gerald can run the 400-meter hurdles with his improved diet. His speed improved by 10%, so we need to find 110% of his original time. The original time for Gerald is 40 seconds, so the improved time is $40\times 1.1=44$ seconds. </think>
<answer> 44 </answer>
CPPO (87.5%) Response ✓:
<think> Let’s break down the problem step by step. First, we need to determine Gerald’s original time to run the 400-meter hurdles. We know that Lee is two seconds faster than Gerald, and Lee runs the 400-meter hurdles in 38 seconds. Therefore, Gerald’s original time is $38+2=40$ seconds. Next, we need to calculate Gerald’s improved speed after his diet. Gerald’s diet improved his speed by 10%. To find his improved time, we need to calculate 10% of 40 seconds and subtract it from 40 seconds. 10% of 40 is $40\times 0.10=4$ seconds. So, Gerald’s improved time is $40-4=36$ seconds. </think>
<answer> 36 </answer>
GRPO (93.75%) Response ✓:
<think> Lee used to be able to run the 400-meter hurdles two seconds faster than Gerald would run the 400-meter hurdles. So, if Lee runs in 38 seconds, Gerald used to run in $38+2=40$ seconds. Gerald’s diet improved his speed by 10%. To find out how fast Gerald can run now, we need to calculate 10% of 40 seconds and subtract it from 40 seconds. 10% of 40 seconds is $0.10\times 40=4$ seconds. So, Gerald’s new time is $40-4=36$ seconds. </think>
<answer> 36 </answer>
REFERENCES
 * [1]
   Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al.
   Gpt-4 technical report, 2023.
 * [2]
   Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, Fei Huang, et al.
   Qwen technical report.
   arXiv preprint arXiv:2309.16609, 2023.
 * [3]
   Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman.
   Training verifiers to solve math word problems.
   arXiv preprint arXiv:2110.14168, 2021.
 * [4]
   Hugging Face.
   Open r1: A fully open reproduction of deepseek-r1, January 2025.
 * [5]
   Xinyu Guan, Li Lyna Zhang, Yifei Liu, Ning Shang, Youran Sun, Yi Zhu, Fan Yang, and Mao Yang.
   rstar-math: Small llms can master math reasoning with self-evolved deep thinking.
   arXiv preprint arXiv:2501.04519, 2025.
 * [6]
   Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al.
   Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning.
   arXiv preprint arXiv:2501.12948, 2025.
 * [7]
   Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt.
   Measuring mathematical problem solving with the math dataset.
   In NeurIPS, 2021.
 * [8]
   Jingcheng Hu, Yinmin Zhang, Qi Han, Daxin Jiang, and Heung-Yeung Shum Xiangyu Zhang.
   Open-reasoner-zero: An open source approach to scaling reinforcement learning on the base model, 2025.
 * [9]
   Aaron Jaech, Adam Kalai, Adam Lerer, Adam Richardson, Ahmed El-Kishky, Aiden Low, Alec Helyar, Aleksander Madry, Alex Beutel, Alex Carney, et al.
   Openai o1 system card.
   arXiv preprint arXiv:2412.16720, 2024.
 * [10]
   Naman Jain, King Han, Alex Gu, Wen-Ding Li, Fanjia Yan, Tianjun Zhang, Sida Wang, Armando Solar-Lezama, Koushik Sen, and Ion Stoica.
   Livecodebench: Holistic and contamination free evaluation of large language models for code.
   arXiv preprint arXiv:2403.07974, 2024.
 * [11]
   Yu Kang, Xianghui Sun, Liangyu Chen, and Wei Zou.
   C3ot: Generating shorter chain-of-thought without compromising effectiveness.
   arXiv preprint arXiv:2412.11664, 2024.
 * [12]
   Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph Gonzalez, Hao Zhang, and Ion Stoica.
   Efficient memory management for large language model serving with pagedattention.
   In SOSP, 2023.
 * [13]
   Mathematical Association of America.
   American mathematics competitions, 2023.
 * [14]
   Mathematical Association of America.
   American invitational mathematics examination, 2024.
 * [15]
   David Rein, Betty Li Hou, Asa Cooper Stickland, Jackson Petty, Richard Yuanzhe Pang, Julien Dirani, Julian Michael, and Samuel R Bowman.
   Gpqa: A graduate-level google-proof q&a benchmark.
   In COLM, 2024.
 * [16]
   John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov.
   Proximal policy optimization algorithms.
   arXiv preprint arXiv:1707.06347, 2017.
 * [17]
   Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, YK Li, Y Wu, et al.
   Deepseekmath: Pushing the limits of mathematical reasoning in open language models.
   arXiv preprint arXiv:2402.03300, 2024.
 * [18]
   Charlie Snell, Jaehoon Lee, Kelvin Xu, and Aviral Kumar.
   Scaling llm test-time compute optimally can be more effective than scaling model parameters.
   arXiv preprint arXiv:2408.03314, 2024.
 * [19]
   Gemini Team, Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew M Dai, Anja Hauth, et al.
   Gemini: A family of highly capable multimodal models, 2023.
 * [20]
   Kimi Team, Angang Du, Bofei Gao, Bowei Xing, Changjiu Jiang, Cheng Chen, Cheng Li, Chenjun Xiao, Chenzhuang Du, Chonghua Liao, et al.
   Kimi k1. 5: Scaling reinforcement learning with llms.
   arXiv preprint arXiv:2501.12599, 2025.
 * [21]
   Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al.
   Llama: Open and efficient foundation language models, 2023.
 * [22]
   Heming Xia, Yongqi Li, Chak Tou Leong, Wenjie Wang, and Wenjie Li.
   Tokenskip: Controllable chain-of-thought compression in llms.
   arXiv preprint arXiv:2502.12067, 2025.
 * [23]
   Tian Xie, Zitian Gao, Qingnan Ren, Haoming Luo, Yuqian Hong, Bryan Dai, Joey Zhou, Kai Qiu, Zhirong Wu, and Chong Luo.
   Logic-rl: Unleashing llm reasoning with rule-based reinforcement learning.
   arXiv preprint arXiv:2502.14768, 2025.
 * [24]
   An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, et al.
   Qwen2. 5 technical report.
   arXiv preprint arXiv:2412.15115, 2024.
Generated on Fri Mar 28 11:27:05 2025 by LaTeXMLMascot Sammy [data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==] [http://dlmf.nist.gov/LaTeXML/]